{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMB6apOMdL6lelTRImapLCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickymultani/RAG-Testing-Gemma7B-IT/blob/main/RAG_Gemma7B_IT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U -q \"transformers==4.38.1\" --upgrade"
      ],
      "metadata": {
        "id": "lx2NaXfr3C2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q llama-index\n",
        "!pip install -q gradio\n",
        "!pip install -q einops\n",
        "!pip install -q accelerate\n",
        "!pip install -q llama-index-llms-huggingface\n",
        "!pip install -q llama-index-embeddings-fastembed\n",
        "!pip install -q fastembed"
      ],
      "metadata": {
        "id": "3kiH95SuTwGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import Settings\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/Data\").load_data()"
      ],
      "metadata": {
        "id": "vs122vqGTsBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "\n",
        "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "id": "L_oIkSxHIgAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "system_prompt = \"You are a NASA Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
        "\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "ReohpVquTmeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=8192,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 1.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"google/gemma-7b-it\",\n",
        "    model_name=\"google/gemma-7b-it\",\n",
        "    device_map=\"cuda\",\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 512"
      ],
      "metadata": {
        "id": "a-jhOJtQ2ijT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "7x13lpwiTiXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "\n",
        "def predict(input, history):\n",
        "  response = query_engine.query(input)\n",
        "  return str(response)"
      ],
      "metadata": {
        "id": "IlLCVBsbTdQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "def predict(input, history):\n",
        "    start_time = time.time()  # Start the timer\n",
        "\n",
        "    response = query_engine.query(input)  # Process the query\n",
        "\n",
        "    end_time = time.time()  # Stop the timer\n",
        "    response_time = end_time - start_time  # Calculate the time taken\n",
        "\n",
        "    # Format the response to include the time taken\n",
        "    timed_response = f\"{response}\\n\\n(Response Time: {response_time:.2f} seconds)\"\n",
        "    return str(timed_response)\n",
        "\n",
        "# Launch gradio chat ui\n",
        "gr.ChatInterface(predict).launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "Ribxa689r-IP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}